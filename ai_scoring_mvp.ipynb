{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7_6IyLdarrM"
   },
   "source": [
    "# AI-модуль (LLM-based scoring)\n",
    "\n",
    "## Цель работы:\n",
    "\n",
    "Разработать прототип AI-модуля, который оценивает, насколько кандидат подходит под конкретную вакансию,  \n",
    "и возвращает **строго структурированный, объяснимый и воспроизводимый скоринг**.\n",
    "\n",
    "Ключевая задача - уйти от:\n",
    "- поиска по ключевым словам без понимания контекста,\n",
    "- нестабильных LLM-ответов,\n",
    "- галлюцинаций (придуманный опыт, навыки, соответствия).\n",
    "\n",
    "Решение ориентировано на использование в HR-сценариях, где важны:\n",
    "- аргументация оценки,\n",
    "- доверие к результату,\n",
    "- возможность Human-in-the-Loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wohd3bMW0Ve5"
   },
   "outputs": [],
   "source": [
    "!pip -q install openai datasets pandas numpy scikit-learn jsonschema tiktoken tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-iL7SLLbFYm"
   },
   "source": [
    "В рамках прототипа используются Оpenai - доступ к LLM API\n",
    "\n",
    "Окружение: Google Colab.\n",
    "\n",
    "API-ключ передаётся через переменные окружения (Colab Secrets),  \n",
    "что соответствует базовым требованиям безопасности и production-подходу.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "KoxTdOo20lmt"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"OPENAI_API_KEY not found in Colab Secrets\")\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "NSEA3QYNMuTy"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from jsonschema import validate, ValidationError\n",
    "import tiktoken\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "PROMPT_VERSION = \"v1.0\"\n",
    "TEMPERATURE = 0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QxYZ6A1YbsDR"
   },
   "source": [
    "## Строгий ответ (JSON Schema)\n",
    "\n",
    "Для минимизации галлюцинаций и повышения надёжности:\n",
    "- финальный ответ LLM должен строго соответствовать JSON Schema,\n",
    "- при несоответствии выполняется автоматическая попытка repair\n",
    "\n",
    "Это позволяет:\n",
    "- безопасно использовать результат в downstream-системах,\n",
    "- гарантировать стабильный формат для HR и аналитиков.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "TPJ54DKU1pht"
   },
   "outputs": [],
   "source": [
    "AI_SCORING_SCHEMA = {\n",
    "  \"type\": \"object\",\n",
    "  \"required\": [\"prompt_version\",\"final_score\",\"evaluation\",\"missing_info\",\"contradictions\",\"meta\"],\n",
    "  \"properties\": {\n",
    "    \"prompt_version\": {\"type\":\"string\"},\n",
    "    \"final_score\": {\"type\":\"number\", \"minimum\":0, \"maximum\":100},\n",
    "    \"evaluation\": {\n",
    "      \"type\":\"array\",\n",
    "      \"items\":{\n",
    "        \"type\":\"object\",\n",
    "        \"required\":[\"req_id\",\"requirement\",\"priority\",\"status\",\"evidence\",\"reason\",\"score_contribution\"],\n",
    "        \"properties\":{\n",
    "          \"req_id\":{\"type\":\"string\"},\n",
    "          \"requirement\":{\"type\":\"string\"},\n",
    "          \"priority\":{\"type\":\"string\", \"enum\":[\"must\",\"should\"]},\n",
    "          \"status\":{\"type\":\"string\", \"enum\":[\"supported\",\"contradicted\",\"missing\",\"unclear\"]},\n",
    "          \"evidence\":{\n",
    "            \"type\":\"array\",\n",
    "            \"items\":{\n",
    "              \"type\":\"object\",\n",
    "              \"required\":[\"quote\"],\n",
    "              \"properties\":{\"quote\":{\"type\":\"string\"}}\n",
    "            }\n",
    "          },\n",
    "          \"reason\":{\"type\":\"string\"},\n",
    "          \"score_contribution\":{\"type\":\"number\"}\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"missing_info\":{\"type\":\"array\", \"items\":{\"type\":\"string\"}},\n",
    "    \"contradictions\":{\"type\":\"array\", \"items\":{\"type\":\"string\"}},\n",
    "    \"meta\":{\n",
    "      \"type\":\"object\",\n",
    "      \"required\":[\"latency_ms\",\"input_tokens\",\"output_tokens\",\"model\",\"n_llm_calls\",\"json_valid\",\"json_repaired\"],\n",
    "      \"properties\":{\n",
    "        \"latency_ms\":{\"type\":\"number\"},\n",
    "        \"input_tokens\":{\"type\":\"number\"},\n",
    "        \"output_tokens\":{\"type\":\"number\"},\n",
    "        \"model\":{\"type\":\"string\"},\n",
    "        \"n_llm_calls\":{\"type\":\"number\"},\n",
    "        \"json_valid\":{\"type\":\"number\"},\n",
    "        \"json_repaired\":{\"type\":\"number\"}\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dKkJ33Mb_SG"
   },
   "source": [
    "## Fairness и защита от bias\n",
    "\n",
    "Перед передачей данных в LLM выполняется минимальная очистка текста:\n",
    "- удаление email, телефонов, URL,\n",
    "- маскирование дат рождения и гендерных маркеров.\n",
    "\n",
    "Это снижает риск:\n",
    "- дискриминации по полу и возрасту,\n",
    "- утечки персональных данных,\n",
    "- смещения модели в сторону нерелевантных признаков.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ULqsM99nNJkg"
   },
   "outputs": [],
   "source": [
    "# --- PII/Fairness sanitization (минимально, но надежно) ---\n",
    "_email_re = re.compile(r\"\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b\")\n",
    "_phone_re = re.compile(r\"\\+?\\d[\\d\\s\\-\\(\\)]{8,}\\d\")\n",
    "_url_re   = re.compile(r\"https?://\\S+\")\n",
    "_birth_re = re.compile(r\"\\b(\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4})\\b\")\n",
    "_gender_re = re.compile(r\"\\b(мужск(ой|ая)|женск(ий|ая))\\b\", re.IGNORECASE)\n",
    "\n",
    "def sanitize_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Удаляю/маскирую потенциальные PII и защищённые признаки.\n",
    "    Это снижает риск bias и делает пайплайн ближе к production-подходу.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = _email_re.sub(\"[EMAIL]\", s)\n",
    "    s = _phone_re.sub(\"[PHONE]\", s)\n",
    "    s = _url_re.sub(\"[URL]\", s)\n",
    "    s = _birth_re.sub(\"[DATE]\", s)\n",
    "    s = _gender_re.sub(\"[GENDER]\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def count_tokens(text: str, model: str = MODEL) -> int:\n",
    "    try:\n",
    "        enc = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        enc = tiktoken.get_encoding(\"o200k_base\")\n",
    "    return len(enc.encode(text or \"\"))\n",
    "\n",
    "def chat_json(system: str, user: str, temperature: float = TEMPERATURE):\n",
    "    \"\"\"\n",
    "    Вызов LLM с принудительным возвратом JSON.\n",
    "    Метрики токенов берём из usage (это основной источник правды).\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ],\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "    )\n",
    "    latency_ms = (time.time() - t0) * 1000\n",
    "    content = resp.choices[0].message.content\n",
    "    data = json.loads(content)\n",
    "    in_tok = resp.usage.prompt_tokens\n",
    "    out_tok = resp.usage.completion_tokens\n",
    "    return data, latency_ms, in_tok, out_tok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQ1qLIYnOHTx",
    "outputId": "bcea17d7-c074-467c-b6e5-59cfc0551d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер датасета: (150493, 91)\n",
      "Распределение целевой переменной cv_status:\n",
      "cv_status\n",
      "Отказ          129503\n",
      "Приглашение     20990\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Загрузка основного датасета.\n",
    "# Формат файла: train.csv, разделитель '|'\n",
    "df = pd.read_csv(\n",
    "    \"train.csv\",\n",
    "    sep=\"|\",\n",
    "    engine=\"python\",\n",
    "    on_bad_lines=\"skip\"\n",
    ")\n",
    "\n",
    "# Проверяем наличие обязательных колонок,\n",
    "# без которых невозможно корректно сопоставлять резюме и вакансии\n",
    "required_cols = [\"idCv\", \"idVacancy\", \"cv_status\"]\n",
    "for c in required_cols:\n",
    "    if c not in df.columns:\n",
    "        raise ValueError(f\"В датасете отсутствует обязательная колонка: {c}\")\n",
    "\n",
    "# Базовая диагностика датасета\n",
    "print(\"Размер датасета:\", df.shape)\n",
    "print(\"Распределение целевой переменной cv_status:\")\n",
    "print(df[\"cv_status\"].value_counts(dropna=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoOcVe8ncWw5"
   },
   "source": [
    "## Формирование текстов резюме и вакансий\n",
    "\n",
    "Исходные поля датасета имеют сложную структуру  \n",
    "(строки, JSON, списки, частично заполненные значения).\n",
    "\n",
    "Для LLM:\n",
    "- резюме и вакансия собираются в **единый нормализованный текст**,\n",
    "- используются только потенциально релевантные поля,\n",
    "- пустые и шумовые значения отбрасываются.\n",
    "\n",
    "Это позволяет модели работать с чистым, читаемым контекстом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5yrgDJpOa4L",
    "outputId": "9011bce0-7ac4-4164-d530-60f6f3f42aba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty resume_text rate: 6.644827334161722e-06\n",
      "empty vacancy_text rate: 0.0\n",
      "resume_text len stats:\n",
      " count    150493.000000\n",
      "mean        397.856425\n",
      "std         399.449194\n",
      "min          36.000000\n",
      "25%         189.000000\n",
      "50%         278.000000\n",
      "75%         453.000000\n",
      "max       11667.000000\n",
      "Name: resume_text, dtype: float64\n",
      "vacancy_text len stats:\n",
      " count    150493.000000\n",
      "mean       1111.171868\n",
      "std         842.517205\n",
      "min         114.000000\n",
      "25%         558.000000\n",
      "50%         870.000000\n",
      "75%        1409.000000\n",
      "max       25614.000000\n",
      "Name: vacancy_text, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "PII_COLS = {\n",
    "    \"birthday\",\"gender\",\"age\",\"locality\",\"localityName\",\n",
    "    \"vacancyAddress\",\"vacancyAddressHouse\",\"vacancyAddressAdditionalInfo\",\n",
    "    \"geo\",\"contactPerson\",\"contactSource\",\"company\",\"fullCompanyName\",\n",
    "    \"addressCode\",\"addressOffice\",\"contactList\"\n",
    "}\n",
    "\n",
    "def safe_str(x):\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"\"\n",
    "    s = str(x).strip()\n",
    "    return \"\" if s.lower() == \"nan\" else s\n",
    "\n",
    "def try_json(x):\n",
    "    s = safe_str(x)\n",
    "    if not s:\n",
    "        return None\n",
    "    if isinstance(x, (list, dict)):\n",
    "        return x\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def list_to_text(x, key=None, limit=30):\n",
    "    \"\"\"\n",
    "    Превращаю json-колонки (списки/словари) в компактный текст.\n",
    "    Это облегчает LLM работу и делает данные единообразными.\n",
    "    \"\"\"\n",
    "    obj = try_json(x)\n",
    "    if obj is None:\n",
    "        return safe_str(x)\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        items = []\n",
    "        for it in obj[:limit]:\n",
    "            if isinstance(it, dict):\n",
    "                if key and it.get(key):\n",
    "                    items.append(str(it[key]))\n",
    "                else:\n",
    "                    vals = [str(v) for v in it.values() if v is not None][:2]\n",
    "                    if vals:\n",
    "                        items.append(\" / \".join(vals))\n",
    "            else:\n",
    "                items.append(str(it))\n",
    "        return \", \".join([i for i in items if i])\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        pairs = []\n",
    "        for k, v in list(obj.items())[:limit]:\n",
    "            if v is None:\n",
    "                continue\n",
    "            pairs.append(f\"{k}: {v}\")\n",
    "        return \"; \".join(pairs)\n",
    "\n",
    "    return safe_str(x)\n",
    "\n",
    "def build_resume_text(row) -> str:\n",
    "    parts = []\n",
    "    parts.append(f\"Желаемая должность: {safe_str(row.get('positionName',''))}\")\n",
    "    parts.append(f\"Навыки: {list_to_text(row.get('skills_cv',''))}\")\n",
    "    parts.append(f\"Hard skills: {list_to_text(row.get('hardSkills_cv',''))}\")\n",
    "    parts.append(f\"Soft skills: {list_to_text(row.get('softSkills_cv',''))}\")\n",
    "    parts.append(f\"Опыт (лет): {safe_str(row.get('experience',''))}\")\n",
    "    parts.append(f\"Опыт работы: {list_to_text(row.get('workExperienceList',''))}\")\n",
    "    parts.append(f\"Образование: {list_to_text(row.get('educationList',''))} {safe_str(row.get('education',''))}\".strip())\n",
    "    parts.append(f\"Языки: {list_to_text(row.get('languageKnowledge_cv',''))}\")\n",
    "    parts = [p for p in parts if p.split(\": \",1)[-1].strip()]\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "def build_vacancy_text(row) -> str:\n",
    "    parts = []\n",
    "    parts.append(f\"Вакансия: {safe_str(row.get('vacancyName',''))}\")\n",
    "    parts.append(f\"Квалификации/требования: {safe_str(row.get('qualifications',''))}\")\n",
    "    parts.append(f\"Доп. требования: {safe_str(row.get('additionalRequirements',''))}\")\n",
    "    parts.append(f\"Требования к позиции: {safe_str(row.get('positionRequirements',''))}\")\n",
    "    parts.append(f\"Обязанности: {safe_str(row.get('responsibilities',''))}\")\n",
    "    parts.append(f\"Условия: {safe_str(row.get('conditions',''))}\")\n",
    "    parts.append(f\"Hard skills: {list_to_text(row.get('hardSkills_vacancy',''))}\")\n",
    "    parts.append(f\"Soft skills: {list_to_text(row.get('softSkills_vacancy',''))}\")\n",
    "    parts.append(f\"Skills: {list_to_text(row.get('skills_vacancy',''))}\")\n",
    "    parts.append(f\"Требуемый опыт: {safe_str(row.get('experienceRequirements',''))}\")\n",
    "    parts.append(f\"График: {safe_str(row.get('scheduleType_vacancy',''))}\")\n",
    "    parts.append(f\"Требования к образованию: {list_to_text(row.get('educationRequirements',''))}\")\n",
    "    parts = [p for p in parts if p.split(\": \",1)[-1].strip()]\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "df[\"resume_text\"] = df.apply(build_resume_text, axis=1).map(sanitize_text)\n",
    "df[\"vacancy_text\"] = df.apply(build_vacancy_text, axis=1).map(sanitize_text)\n",
    "\n",
    "print(\"empty resume_text rate:\", (df[\"resume_text\"].str.len() < 50).mean())\n",
    "print(\"empty vacancy_text rate:\", (df[\"vacancy_text\"].str.len() < 50).mean())\n",
    "print(\"resume_text len stats:\\n\", df[\"resume_text\"].str.len().describe())\n",
    "print(\"vacancy_text len stats:\\n\", df[\"vacancy_text\"].str.len().describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tP9FE4Eycg-1"
   },
   "source": [
    "## LLM-пайплайн и промпт-дизайн\n",
    "\n",
    "Пайплайн состоит из двух логически разделённых шагов:\n",
    "\n",
    "1. **Извлечение требований вакансии**\n",
    "   - только по тексту вакансии,\n",
    "   - без домысливания,\n",
    "   - с приоритетами `must / should`.\n",
    "\n",
    "2. **Скоринг резюме относительно требований**\n",
    "   - каждый пункт оценивается отдельно,\n",
    "   - статус возможен только при наличии цитаты,\n",
    "   - итоговый балл агрегируется объяснимо.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "kfq5zAbJOm0_"
   },
   "outputs": [],
   "source": [
    "MAX_RESUME_CHARS = 4000\n",
    "MAX_VACANCY_CHARS = 5000\n",
    "\n",
    "def clip_text(s: str, limit: int) -> str:\n",
    "    s = s or \"\"\n",
    "    return s[:limit]\n",
    "\n",
    "SYSTEM_EXTRACT_RU = \"\"\"Ты извлекаешь требования из текста вакансии строго по тексту.\n",
    "Запрещено добавлять требования, которых нет в вакансии.\n",
    "Верни только JSON.\n",
    "\"\"\"\n",
    "\n",
    "USER_EXTRACT_TMPL_RU = \"\"\"ТЕКСТ_ВАКАНСИИ:\n",
    "{vacancy}\n",
    "\n",
    "Задача:\n",
    "Выдели ключевые требования к кандидату (только то, что явно указано).\n",
    "Сфокусируйся на:\n",
    "- Hard skills / технологии / инструменты\n",
    "- Опыт (лет, уровень)\n",
    "- Ограничения (версии, обязательные условия)\n",
    "- Образование / график (если явно важно)\n",
    "\n",
    "Верни JSON строго в формате:\n",
    "{{\n",
    "  \"requirements\":[\n",
    "    {{\n",
    "      \"req_id\":\"R1\",\n",
    "      \"requirement\":\"кратко и конкретно\",\n",
    "      \"priority\":\"must|should\",\n",
    "      \"weight\":0-1\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Правила:\n",
    "- \"must\" ставь только если требование явно обязательное (\"обязательно\", \"требуется\", \"must have\").\n",
    "- Если детали нет — не придумывай.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_SCORE_RU = \"\"\"Ты модуль скоринга резюме относительно вакансии.\n",
    "\n",
    "КРИТИЧЕСКИЕ ПРАВИЛА (анти-галлюцинации):\n",
    "1) Статус \"supported\" или \"contradicted\" можно ставить ТОЛЬКО если есть цитата из резюме (evidence.quote).\n",
    "2) Если цитаты нет — ставь \"missing\" или \"unclear\".\n",
    "3) Игнорируй и НЕ используй защищённые признаки (пол, возраст, имя, национальность, адрес), даже если они присутствуют.\n",
    "4) Не считай противоречием близкие уровни образования без явного запрета в вакансии (например, \"среднее\" vs \"среднее профессиональное\").\n",
    "\n",
    "Верни только JSON.\n",
    "\"\"\"\n",
    "\n",
    "USER_SCORE_TMPL_RU = \"\"\"ТЕКСТ_ВАКАНСИИ:\n",
    "{vacancy}\n",
    "\n",
    "ТРЕБОВАНИЯ_JSON:\n",
    "{req_json}\n",
    "\n",
    "ТЕКСТ_РЕЗЮМЕ:\n",
    "{resume}\n",
    "\n",
    "Верни JSON со следующими полями:\n",
    "- prompt_version\n",
    "- final_score (0..100)\n",
    "- evaluation: по каждому требованию:\n",
    "  req_id, requirement, priority,\n",
    "  status (supported/contradicted/missing/unclear),\n",
    "  evidence: список объектов {{quote}},\n",
    "  reason (коротко, понятно HR),\n",
    "  score_contribution (число)\n",
    "- missing_info: список важных данных, которых не хватает в резюме\n",
    "- contradictions: список противоречий\n",
    "- meta: latency_ms/input_tokens/output_tokens/model/n_llm_calls/json_valid/json_repaired (поставь 0 — код перезапишет)\n",
    "\n",
    "Гайд по скорингу:\n",
    "- must требования определяют итог сильнее.\n",
    "- contradicted must => сильный минус.\n",
    "- missing must => минус, но мягче.\n",
    "\"\"\"\n",
    "\n",
    "SYSTEM_REPAIR_RU = \"\"\"Исправь JSON так, чтобы он строго соответствовал JSON Schema.\n",
    "Нельзя добавлять новую информацию — только сделать JSON валидным.\n",
    "Верни только JSON.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnRdYAJAcuma"
   },
   "source": [
    "## Основная функция ai_scoring\n",
    "\n",
    "Функция ai_scoring:\n",
    "- принимает текст вакансии и резюме,\n",
    "- возвращает структурированный JSON-скоринг,\n",
    "- инкапсулирует весь LLM-пайплайн.\n",
    "\n",
    "Внутри:\n",
    "- кэширование требований вакансии,\n",
    "- контроль токенов и latency,\n",
    "- строгая JSON-валидация,\n",
    "- автоматический repair при необходимости.\n",
    "\n",
    "Функция является центральным интерфейсом модуля.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "7EVhJJ0UOzWS"
   },
   "outputs": [],
   "source": [
    "_req_cache = {}\n",
    "\n",
    "def enforce_no_hallucination(out: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Code-level safeguard:\n",
    "    supported/contradicted допустимы только при наличии evidence.quote.\n",
    "    Это защищает от галлюцинаций даже при ошибке модели.\n",
    "    \"\"\"\n",
    "    for item in out.get(\"evaluation\", []):\n",
    "        status = item.get(\"status\")\n",
    "        evidence = item.get(\"evidence\") or []\n",
    "        has_quote = any(isinstance(e, dict) and (e.get(\"quote\") or \"\").strip() for e in evidence)\n",
    "        if status in {\"supported\", \"contradicted\"} and not has_quote:\n",
    "            item[\"status\"] = \"unclear\"\n",
    "            item[\"score_contribution\"] = 0\n",
    "            item[\"reason\"] = (item.get(\"reason\",\"\").strip() + \" (нет подтверждающей цитаты в резюме)\").strip()\n",
    "            item[\"evidence\"] = []\n",
    "    return out\n",
    "\n",
    "def ai_scoring(vacancy_text: str, resume_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Main API function for the test:\n",
    "    input: raw vacancy_text + resume_text\n",
    "    output: strict JSON scoring according to schema\n",
    "    \"\"\"\n",
    "    vacancy = clip_text(sanitize_text(vacancy_text), MAX_VACANCY_CHARS)\n",
    "    resume  = clip_text(sanitize_text(resume_text),  MAX_RESUME_CHARS)\n",
    "\n",
    "    total_latency = 0.0\n",
    "    total_in = 0\n",
    "    total_out = 0\n",
    "    n_calls = 0\n",
    "\n",
    "    vkey = hash(vacancy)\n",
    "\n",
    "    # 1) Extract requirements (cached per vacancy text)\n",
    "    if vkey not in _req_cache:\n",
    "        req_data, lat, tin, tout = chat_json(\n",
    "            SYSTEM_EXTRACT_RU,\n",
    "            USER_EXTRACT_TMPL_RU.format(vacancy=vacancy),\n",
    "            temperature=0.0\n",
    "        )\n",
    "        _req_cache[vkey] = req_data\n",
    "        total_latency += lat; total_in += tin; total_out += tout; n_calls += 1\n",
    "\n",
    "    req_data = _req_cache[vkey]\n",
    "    req_json = json.dumps(req_data, ensure_ascii=False)\n",
    "\n",
    "    # 2) Score candidate vs extracted requirements\n",
    "    out_data, lat, tin, tout = chat_json(\n",
    "        SYSTEM_SCORE_RU,\n",
    "        USER_SCORE_TMPL_RU.format(vacancy=vacancy, req_json=req_json, resume=resume),\n",
    "        temperature=0.0\n",
    "    )\n",
    "    total_latency += lat; total_in += tin; total_out += tout; n_calls += 1\n",
    "\n",
    "    # 3) Enforce anti-hallucination on code level\n",
    "    out_data = enforce_no_hallucination(out_data)\n",
    "\n",
    "    # 4) Fill meta + validate/repair\n",
    "    out_data[\"prompt_version\"] = PROMPT_VERSION\n",
    "    out_data[\"meta\"] = {\n",
    "        \"latency_ms\": float(total_latency),\n",
    "        \"input_tokens\": float(total_in),\n",
    "        \"output_tokens\": float(total_out),\n",
    "        \"model\": MODEL,\n",
    "        \"n_llm_calls\": float(n_calls),\n",
    "        \"json_valid\": 1.0,\n",
    "        \"json_repaired\": 0.0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        validate(instance=out_data, schema=AI_SCORING_SCHEMA)\n",
    "        return out_data\n",
    "    except ValidationError:\n",
    "        # one repair attempt\n",
    "        repair_user = (\n",
    "            \"INVALID_JSON:\\n\" + json.dumps(out_data, ensure_ascii=False) +\n",
    "            \"\\n\\nSCHEMA:\\n\" + json.dumps(AI_SCORING_SCHEMA, ensure_ascii=False)\n",
    "        )\n",
    "        fixed, lat2, tin2, tout2 = chat_json(SYSTEM_REPAIR_RU, repair_user, temperature=0.0)\n",
    "        total_latency += lat2; total_in += tin2; total_out += tout2; n_calls += 1\n",
    "\n",
    "        fixed = enforce_no_hallucination(fixed)\n",
    "\n",
    "        fixed[\"prompt_version\"] = PROMPT_VERSION\n",
    "        fixed[\"meta\"] = {\n",
    "            \"latency_ms\": float(total_latency),\n",
    "            \"input_tokens\": float(total_in),\n",
    "            \"output_tokens\": float(total_out),\n",
    "            \"model\": MODEL,\n",
    "            \"n_llm_calls\": float(n_calls),\n",
    "            \"json_valid\": 0.0,\n",
    "            \"json_repaired\": 1.0\n",
    "        }\n",
    "        validate(instance=fixed, schema=AI_SCORING_SCHEMA)\n",
    "        return fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVUghTDFO3p7",
    "outputId": "b496019f-7ed6-4dd1-eb25-bba11bfc3a80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_score: 25\n",
      "n_eval: 4\n",
      "{\n",
      "  \"prompt_version\": \"v1.0\",\n",
      "  \"final_score\": 25,\n",
      "  \"evaluation\": [\n",
      "    {\n",
      "      \"req_id\": \"R1\",\n",
      "      \"requirement\": \"Наличие опыта работы\",\n",
      "      \"priority\": \"should\",\n",
      "      \"status\": \"supported\",\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"quote\": \"Опыт (лет): 2\"\n",
      "        }\n",
      "      ],\n",
      "      \"reason\": \"Кандидат имеет 2 года опыта работы.\",\n",
      "      \"score_contribution\": 50\n",
      "    },\n",
      "    {\n",
      "      \"req_id\": \"R2\",\n",
      "      \"requirement\": \"Среднее профессиональное образование\",\n",
      "      \"priority\": \"should\",\n",
      "      \"status\": \"contradicted\",\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"quote\": \"Образование: 2015 / ТСОШ Среднее\"\n",
      "        }\n",
      "      ],\n",
      "      \"reason\": \"Кандидат имеет среднее общее образование, а требуется среднее профессиональное.\",\n",
      "      \"score_contribution\": -50\n",
      "    },\n",
      "    {\n",
      "      \"req_id\": \"R3\",\n",
      "      \"requirement\": \"Ответственность\",\n",
      "      \"priority\": \"should\",\n",
      "      \"status\": \"missing\",\n",
      "      \"evidence\": [],\n",
      "      \"reason\": \"Нет информации о наличии ответственности.\",\n",
      "      \"score_contribution\": -25\n",
      "    },\n",
      "    {\n",
      "      \"req_id\": \"R4\",\n",
      "      \"requirement\": \"Сменный график работы\",\n",
      "      \"priority\": \"should\",\n",
      "      \"status\": \"missing\",\n",
      "      \"evidence\": [],\n",
      "      \"reason\": \"Нет информации о готовности к сменному графику.\",\n",
      "      \"score_contribution\": -25\n",
      "    }\n",
      "  ],\n",
      "  \"missing_info\": [\n",
      "    \"Информация о наличии ответственности\",\n",
      "    \"Информация о готовности к сменному графику работы\"\n",
      "  ],\n",
      "  \"contradictions\": [],\n",
      "  \"meta\": {\n",
      "    \"latency_ms\": 9240.05675315857,\n",
      "    \"input_tokens\": 822.0,\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# test (одна пара из датасета)\n",
    "row = df.dropna(subset=[\"vacancy_text\",\"resume_text\"]).iloc[0]\n",
    "res = ai_scoring(row[\"vacancy_text\"], row[\"resume_text\"])\n",
    "print(\"final_score:\", res[\"final_score\"])\n",
    "print(\"n_eval:\", len(res[\"evaluation\"]))\n",
    "print(json.dumps(res, ensure_ascii=False, indent=2)[:1500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fffl02PFc8Qo"
   },
   "source": [
    "## Массовый прогон и сбор метрик\n",
    "\n",
    "Для оценки качества решения:\n",
    "- используется 100 пар (резюме, вакансия),\n",
    "- не менее 10 уникальных вакансий,\n",
    "- каждый скоринг выполняется независимо.\n",
    "\n",
    "Собираются:\n",
    "- итоговые оценки,\n",
    "- latency,\n",
    "- входные и выходные токены,\n",
    "- стабильность JSON-ответов.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "51ChWaprAjYy",
    "outputId": "d4b74976-3ecd-4dcf-e670-de302c880e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs: (100, 93) unique vacancies: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring_100: 100%|██████████| 100/100 [34:37<00:00, 20.77s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"results\",\n  \"rows\": 100,\n  \"fields\": [\n    {\n      \"column\": \"idVacancy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 100,\n        \"samples\": [\n          \"4b67a480-e65b-11ee-a22d-295bfdef1967\",\n          \"9f4b9338-0307-11f0-9882-e7d0d2cf29b1\",\n          \"71277c45-11e9-11ef-8f26-cb26dff57dd7\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"idCv\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 98,\n        \"samples\": [\n          \"e32730a0-b0aa-11ef-a4c1-47917b201f31\",\n          \"e9bca3b0-0572-11f0-8dce-e7f46f7514cd\",\n          \"e9c72720-e671-11ee-ac6b-c7bd3a8c3ec7\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cv_status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"\\u041f\\u0440\\u0438\\u0433\\u043b\\u0430\\u0448\\u0435\\u043d\\u0438\\u0435\",\n          \"\\u041e\\u0442\\u043a\\u0430\\u0437\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"final_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21,\n        \"min\": 0,\n        \"max\": 85,\n        \"num_unique_values\": 13,\n        \"samples\": [\n          33,\n          80\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latency_ms\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10832.756939795698,\n        \"min\": 6011.004686355591,\n        \"max\": 70163.82074356079,\n        \"num_unique_values\": 100,\n        \"samples\": [\n          12586.074829101562,\n          20107.296228408813\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 623.5318610281047,\n        \"min\": 946.0,\n        \"max\": 3726.0,\n        \"num_unique_values\": 99,\n        \"samples\": [\n          1608.0,\n          1231.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output_tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 455.9294286832051,\n        \"min\": 237.0,\n        \"max\": 3389.0,\n        \"num_unique_values\": 93,\n        \"samples\": [\n          494.0,\n          504.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"json_valid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 1.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"json_repaired\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_requirements\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 1,\n        \"max\": 24,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          10\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "results"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-975747fb-ae0e-492e-a6a8-83046da3042c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idVacancy</th>\n",
       "      <th>idCv</th>\n",
       "      <th>cv_status</th>\n",
       "      <th>final_score</th>\n",
       "      <th>latency_ms</th>\n",
       "      <th>input_tokens</th>\n",
       "      <th>output_tokens</th>\n",
       "      <th>json_valid</th>\n",
       "      <th>json_repaired</th>\n",
       "      <th>n_requirements</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8a4f1ec5-9005-11ec-a69c-4febb26dc4ec</td>\n",
       "      <td>6807cc00-2703-11ed-81e0-fdf9f86d256a</td>\n",
       "      <td>Отказ</td>\n",
       "      <td>10</td>\n",
       "      <td>28034.849882</td>\n",
       "      <td>3653.0</td>\n",
       "      <td>1589.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10d6da82-9280-11ef-b4d5-e7d0d2cf29b1</td>\n",
       "      <td>aeba9940-f875-11ed-a410-715f91579781</td>\n",
       "      <td>Отказ</td>\n",
       "      <td>50</td>\n",
       "      <td>9257.095814</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>379.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f31ec640-1b09-11ee-9f4d-9586bb63c653</td>\n",
       "      <td>e0db3e20-a0ee-11ef-b469-c7bd3a8c3ec7</td>\n",
       "      <td>Отказ</td>\n",
       "      <td>30</td>\n",
       "      <td>18086.399078</td>\n",
       "      <td>1243.0</td>\n",
       "      <td>779.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7dbbed98-91de-11ef-8716-cb26dff57dd7</td>\n",
       "      <td>c76d2ca0-50a9-11ee-b447-d53cd48f29e6</td>\n",
       "      <td>Отказ</td>\n",
       "      <td>0</td>\n",
       "      <td>17188.035011</td>\n",
       "      <td>1380.0</td>\n",
       "      <td>884.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1d275ea5-fdda-11ec-b057-57fc951f3846</td>\n",
       "      <td>c6d707b0-fb4c-11ec-bcb3-839f0d9a4379</td>\n",
       "      <td>Отказ</td>\n",
       "      <td>60</td>\n",
       "      <td>22299.293995</td>\n",
       "      <td>2432.0</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-975747fb-ae0e-492e-a6a8-83046da3042c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-975747fb-ae0e-492e-a6a8-83046da3042c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-975747fb-ae0e-492e-a6a8-83046da3042c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                              idVacancy                                  idCv  \\\n",
       "0  8a4f1ec5-9005-11ec-a69c-4febb26dc4ec  6807cc00-2703-11ed-81e0-fdf9f86d256a   \n",
       "1  10d6da82-9280-11ef-b4d5-e7d0d2cf29b1  aeba9940-f875-11ed-a410-715f91579781   \n",
       "2  f31ec640-1b09-11ee-9f4d-9586bb63c653  e0db3e20-a0ee-11ef-b469-c7bd3a8c3ec7   \n",
       "3  7dbbed98-91de-11ef-8716-cb26dff57dd7  c76d2ca0-50a9-11ee-b447-d53cd48f29e6   \n",
       "4  1d275ea5-fdda-11ec-b057-57fc951f3846  c6d707b0-fb4c-11ec-bcb3-839f0d9a4379   \n",
       "\n",
       "  cv_status  final_score    latency_ms  input_tokens  output_tokens  \\\n",
       "0     Отказ           10  28034.849882        3653.0         1589.0   \n",
       "1     Отказ           50   9257.095814        1430.0          379.0   \n",
       "2     Отказ           30  18086.399078        1243.0          779.0   \n",
       "3     Отказ            0  17188.035011        1380.0          884.0   \n",
       "4     Отказ           60  22299.293995        2432.0         1040.0   \n",
       "\n",
       "   json_valid  json_repaired  n_requirements  \n",
       "0         1.0            0.0              10  \n",
       "1         1.0            0.0               2  \n",
       "2         1.0            0.0               5  \n",
       "3         1.0            0.0               6  \n",
       "4         1.0            0.0               7  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "pairs_df = (\n",
    "    df.dropna(subset=[\"resume_text\",\"vacancy_text\",\"cv_status\",\"idVacancy\",\"idCv\"])\n",
    "      .sample(n=100, random_state=42)\n",
    "      .copy()\n",
    ")\n",
    "\n",
    "print(\"pairs:\", pairs_df.shape, \"unique vacancies:\", pairs_df[\"idVacancy\"].nunique())\n",
    "\n",
    "rows_out = []\n",
    "for _, row in tqdm(pairs_df.iterrows(), total=len(pairs_df), desc=\"scoring_100\"):\n",
    "    out = ai_scoring(row[\"vacancy_text\"], row[\"resume_text\"])\n",
    "    rows_out.append({\n",
    "        \"idVacancy\": row[\"idVacancy\"],\n",
    "        \"idCv\": row[\"idCv\"],\n",
    "        \"cv_status\": row[\"cv_status\"],\n",
    "        \"final_score\": out[\"final_score\"],\n",
    "        \"latency_ms\": out[\"meta\"][\"latency_ms\"],\n",
    "        \"input_tokens\": out[\"meta\"][\"input_tokens\"],\n",
    "        \"output_tokens\": out[\"meta\"][\"output_tokens\"],\n",
    "        \"json_valid\": out[\"meta\"][\"json_valid\"],\n",
    "        \"json_repaired\": out[\"meta\"][\"json_repaired\"],\n",
    "        \"n_requirements\": len(out.get(\"evaluation\", [])),\n",
    "    })\n",
    "\n",
    "results = pd.DataFrame(rows_out)\n",
    "results.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00qRt5SMdKTA"
   },
   "source": [
    "## Оценка качества скоринга\n",
    "\n",
    "В качестве ground truth используется:\n",
    "- cv_status из датасета,\n",
    "- приглашение → 1, отказ → 0.\n",
    "\n",
    "Рассчитываются:\n",
    "- Accuracy,\n",
    "- Precision,\n",
    "- Recall,\n",
    "- F1-score\n",
    "\n",
    "Метрики считаются для разных порогов скоринга  \n",
    "для анализа trade-off между precision и recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IOXYv2SPu1J",
    "outputId": "08bc973c-6fb4-4fe2-8356-39cecddef0c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TH=50 | acc=0.560 | prec=0.051 | rec=0.222 | f1=0.083\n",
      "TH=60 | acc=0.670 | prec=0.071 | rec=0.222 | f1=0.108\n",
      "TH=70 | acc=0.850 | prec=0.200 | rec=0.222 | f1=0.211\n",
      "\n",
      "PIPELINE STATS (mean/std)\n",
      "latency_ms: (20751.80416584015, 10832.756939795698)\n",
      "input_tokens: (1673.41, 623.5318610281047)\n",
      "output_tokens: (944.05, 455.9294286832051)\n",
      "\n",
      "STABILITY / QUALITY\n",
      "json_valid_rate: 1.0\n",
      "repair_rate: 0.0\n",
      "avg_n_requirements: 6.29\n",
      "class_balance: {0: 91, 1: 9}\n"
     ]
    }
   ],
   "source": [
    "y_true = results[\"cv_status\"].astype(str).str.contains(\"Приглаш\", case=False, na=False).astype(int)\n",
    "\n",
    "def metrics_at_threshold(th: int):\n",
    "    y_pred = (results[\"final_score\"] >= th).astype(int)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "for th in [50, 60, 70]:\n",
    "    acc, prec, rec, f1 = metrics_at_threshold(th)\n",
    "    print(f\"TH={th} | acc={acc:.3f} | prec={prec:.3f} | rec={rec:.3f} | f1={f1:.3f}\")\n",
    "\n",
    "# pipeline stats\n",
    "def mean_std(x):\n",
    "    return float(np.mean(x)), float(np.std(x, ddof=1))\n",
    "\n",
    "print(\"\\nPIPELINE STATS (mean/std)\")\n",
    "print(\"latency_ms:\", mean_std(results[\"latency_ms\"]))\n",
    "print(\"input_tokens:\", mean_std(results[\"input_tokens\"]))\n",
    "print(\"output_tokens:\", mean_std(results[\"output_tokens\"]))\n",
    "\n",
    "# stability stats\n",
    "json_valid_rate = float(results[\"json_valid\"].mean())\n",
    "repair_rate = float(results[\"json_repaired\"].mean())\n",
    "avg_nreq = float(results[\"n_requirements\"].mean())\n",
    "\n",
    "print(\"\\nSTABILITY / QUALITY\")\n",
    "print(\"json_valid_rate:\", json_valid_rate)\n",
    "print(\"repair_rate:\", repair_rate)\n",
    "print(\"avg_n_requirements:\", avg_nreq)\n",
    "print(\"class_balance:\", y_true.value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DwViWQJfe_i"
   },
   "source": [
    "Модель демонстрирует ожидаемое поведение при сильном дисбалансе классов (9% приглашений):\n",
    "\n",
    "При увеличении порога скоринга (TH=50 → TH=70) растёт precision и F1, что говорит о более строгом и аккуратном отборе кандидатов.\n",
    "\n",
    "Recall остаётся стабильным (~0.22).\n",
    "\n",
    "Accuracy существенно увеличивается при высоких порогах, что ожидаемо при доминирующем классе отказов и не используется как основная метрика.\n",
    "\n",
    "Оптимальным порогом для сценария предварительного отбора кандидатов является диапазон TH ≈ 60–70, где достигается баланс между точностью и консервативностью скоринга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJekSlaXPz3D",
    "outputId": "81533382-4ef9-4d45-a484-75e8cb997bcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: results_v1.0.csv pairs_v1.0.csv\n"
     ]
    }
   ],
   "source": [
    "results.to_csv(f\"results_{PROMPT_VERSION}.csv\", index=False)\n",
    "pairs_df[[\"idCv\",\"idVacancy\",\"cv_status\"]].to_csv(f\"pairs_{PROMPT_VERSION}.csv\", index=False)\n",
    "print(\"saved:\", f\"results_{PROMPT_VERSION}.csv\", f\"pairs_{PROMPT_VERSION}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaVs9jm1dXdZ"
   },
   "source": [
    "## Калибровка порога принятия решения\n",
    "\n",
    "Так как скоринг непрерывен (0–100),\n",
    "подбирается оптимальный порог по метрике F1.\n",
    "\n",
    "Это демонстрирует:\n",
    "- осознанный подход к принятию решений,\n",
    "- возможность адаптации под разные бизнес-сценарии\n",
    "  (массовый найм vs точечный подбор).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hq-edHcBYXYF",
    "outputId": "c8e8f0a5-f68a-4591-b78e-fe929e801991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST THRESHOLD (by F1):\n",
      "TH=35 | acc=0.550 | prec=0.125 | rec=0.667 | f1=0.211\n"
     ]
    }
   ],
   "source": [
    "best = None\n",
    "for th in range(0, 101, 5):\n",
    "    acc, prec, rec, f1 = metrics_at_threshold(th)\n",
    "    if best is None or f1 > best[-1]:\n",
    "        best = (th, acc, prec, rec, f1)\n",
    "\n",
    "print(\"\\nBEST THRESHOLD (by F1):\")\n",
    "print(f\"TH={best[0]} | acc={best[1]:.3f} | prec={best[2]:.3f} | rec={best[3]:.3f} | f1={best[4]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uzQnxHWOgvk9"
   },
   "source": [
    "В ходе экспериментов были рассмотрены разные пороги принятия решения, соответствующие разным HR-сценариям.\n",
    "\n",
    "TH ≈ 60–70\n",
    "Как мы видим выше, при данных порогах достигается более консервативный режим скоринга:\n",
    "\n",
    "выше precision,\n",
    "\n",
    "ниже количество ложных приглашений.\n",
    "\n",
    "Такой диапазон порогов оптимален для сценария предварительного shortlisting’а, когда AI используется как фильтр для отсечения явно неподходящих кандидатов.\n",
    "\n",
    "TH = 35 (оптимум по F1-score)\n",
    "При калибровке по F1-score оптимальным оказался порог TH = 35, обеспечивающий:\n",
    "\n",
    "высокий recall (0.667),\n",
    "\n",
    "максимальный F1-score (0.211) в условиях сильного дисбаланса классов.\n",
    "\n",
    "Данный порог подходит для сценария recall-first, где важно не упустить потенциально релевантных кандидатов, а финальное решение принимается рекрутером."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9sHSM9mdeL2"
   },
   "source": [
    "## Версионность и воспроизводимость промптов\n",
    "\n",
    "Все используемые промпты:\n",
    "- сохраняются в реестр,\n",
    "- хэшируются,\n",
    "- связываются с версией и моделью.\n",
    "\n",
    "Это позволяет:\n",
    "- отслеживать изменения,\n",
    "- воспроизводить эксперименты,\n",
    "- безопасно развивать систему дальше.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eUjAH2ClZOay",
    "outputId": "afab62e9-5c54-4ae5-8ddc-e5bb60c83ac2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_version': 'v1.0', 'model': 'gpt-4o-mini', 'hashes': {'SYSTEM_EXTRACT_RU': '6fe84c21547b', 'USER_EXTRACT_TMPL_RU': '007ea1c1019d', 'SYSTEM_SCORE_RU': '2679b16ac5c5', 'USER_SCORE_TMPL_RU': '8b438d3dda4a', 'SYSTEM_REPAIR_RU': '748c6504179d'}}\n"
     ]
    }
   ],
   "source": [
    "import hashlib, json\n",
    "\n",
    "PROMPTS = {\n",
    "    \"SYSTEM_EXTRACT_RU\": SYSTEM_EXTRACT_RU,\n",
    "    \"USER_EXTRACT_TMPL_RU\": USER_EXTRACT_TMPL_RU,\n",
    "    \"SYSTEM_SCORE_RU\": SYSTEM_SCORE_RU,\n",
    "    \"USER_SCORE_TMPL_RU\": USER_SCORE_TMPL_RU,\n",
    "    \"SYSTEM_REPAIR_RU\": SYSTEM_REPAIR_RU,\n",
    "}\n",
    "\n",
    "def sha12(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:12]\n",
    "\n",
    "prompt_registry = {\n",
    "    \"prompt_version\": PROMPT_VERSION,\n",
    "    \"model\": MODEL,\n",
    "    \"hashes\": {k: sha12(v) for k, v in PROMPTS.items()},\n",
    "}\n",
    "\n",
    "with open(f\"prompt_registry_{PROMPT_VERSION}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(prompt_registry, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(prompt_registry)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q693euXIh7b5"
   },
   "source": [
    "Пайплайн продемонстрировал устойчивую работу на выборке из 100 пар «вакансия–резюме» с реальными HR-данными и сильным дисбалансом классов (≈9% приглашений).\n",
    "\n",
    "По результатам оценки:\n",
    "\n",
    "при росте порога скоринга увеличивается точность и снижается количество ложных приглашений;\n",
    "\n",
    "при снижении порога значительно растёт recall, что важно для сценариев первичного отбора.\n",
    "\n",
    "Калибровка показала, что:\n",
    "\n",
    "TH ≈ 35 оптимален для recall-oriented сценария (поиск максимума потенциально подходящих кандидатов);\n",
    "\n",
    "TH ≈ 60–70 подходит для консервативного shortlisting’а, когда приоритет — качество приглашений.\n",
    "\n",
    "Таким образом, итоговый скоринг и порог отсечения являются бизнес-настраиваемыми параметрами, а не фиксированными константами, и могут адаптироваться под конкретный HR-процесс без изменения логики модели.\n",
    "\n",
    "Система обеспечивает воспроизводимый, объяснимый и формально валидный результат, что делает её пригодной для использования как базового AI-ядра в задачах автоматизированного подбора персонала.\n",
    "\n",
    "## В дальнейшем решение может быть усилено за счёт:\n",
    "\n",
    "более глубокой и структурированной предобработки данных (нормализация опыта, образования, навыков, унификация форматов списков и JSON-полей);\n",
    "\n",
    "выделения и типизации ключевых сущностей (навыки, технологии, уровни опыта) до этапа LLM-скоринга;\n",
    "\n",
    "использования предварительного фильтра (rule-based или embedding-based) для сокращения числа LLM-вызовов;\n",
    "\n",
    "более точной калибровки весов требований (must/should) под конкретные отрасли и типы вакансий;\n",
    "\n",
    "расширения Human-in-the-Loop сценариев для разметки пограничных случаев и дообучения логики скоринга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Puf8hY4iiH9t"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
